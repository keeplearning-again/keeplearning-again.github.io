<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: December 18, 2023 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.042e26407c9e383d96a1f26d6787c686.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  






<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZV037K7NZ5"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'G-ZV037K7NZ5', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>























  
  
  






  <meta name="author" content="Ruiqiang Xiao" />





  

<meta name="description" content="This blog talks about the historical developement of self-attention mechanism and the implementation of transformer decoder." />



<link rel="alternate" hreflang="en-us" href="https://keeplearning-again.github.io/post/transformer/" />
<link rel="canonical" href="https://keeplearning-again.github.io/post/transformer/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hud72a807ca50d008ad851764b48a793ed_213136_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hud72a807ca50d008ad851764b48a793ed_213136_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  






<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://keeplearning-again.github.io/post/transformer/featured.png" />
<meta property="og:site_name" content="Ruiqiang Xiao&#39;s Personal Webpage" />
<meta property="og:url" content="https://keeplearning-again.github.io/post/transformer/" />
<meta property="og:title" content="Self-attention mechanism and Transformers | Ruiqiang Xiao&#39;s Personal Webpage" />
<meta property="og:description" content="This blog talks about the historical developement of self-attention mechanism and the implementation of transformer decoder." /><meta property="og:image" content="https://keeplearning-again.github.io/post/transformer/featured.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2022-08-12T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2022-08-12T00:00:00&#43;00:00">
  






    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://keeplearning-again.github.io/post/transformer/"
  },
  "headline": "Self-attention mechanism and Transformers",
  
  "image": [
    "https://keeplearning-again.github.io/post/transformer/featured.png"
  ],
  
  "datePublished": "2022-08-12T00:00:00Z",
  "dateModified": "2022-08-12T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Ruiqiang Xiao"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Ruiqiang Xiao's Personal Webpage",
    "logo": {
      "@type": "ImageObject",
      "url": "https://keeplearning-again.github.io/media/icon_hud72a807ca50d008ad851764b48a793ed_213136_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "This blog talks about the historical developement of self-attention mechanism and the implementation of transformer decoder."
}
</script>

  

  




  
  
  

  
  

  


  
  <title>Self-attention mechanism and Transformers | Ruiqiang Xiao&#39;s Personal Webpage</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="77d740a99bda7443b0674a7a636abf6b" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Ruiqiang Xiao&#39;s Personal Webpage</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Ruiqiang Xiao&#39;s Personal Webpage</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Posts</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Publication and Project</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/uploads/resume.pdf"><span>CV</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
            
            <li class="nav-item d-none d-lg-inline-flex">
              <a class="nav-link" href="https://twitter.com/GeorgeCushen" data-toggle="tooltip" data-placement="bottom" title="Follow me on Twitter" target="_blank" rel="noopener" aria-label="Follow me on Twitter">
                <i class="fab fa-twitter" aria-hidden="true"></i>
              </a>
            </li>
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  






















  
  


<div class="article-container pt-3">
  <h1>Self-attention mechanism and Transformers</h1>

  
  <p class="page-subtitle">This blog talks about the historical developement of self-attention mechanism and the implementation of transformer decoder.</p>
  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Ruiqiang Xiao</span>, <span >
      肖睿强</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Aug 12, 2022
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    3 min read
  </span>
  

  
  
  
  

  
  

</div>

  





</div>


<div class="article-header container featured-image-wrapper mt-4 mb-4" style="max-width: 927px; max-height: 713px;">
  <div style="position: relative">
    <img src="/post/transformer/featured_hu55e335c73174c518587033d1e6843012_172075_1200x2500_fit_q75_h2_lanczos_3.webp" width="927" height="713" alt="" class="featured-image">
    <span class="article-header-caption">screenshot of attention mechanism</span>
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <h2 id="1-明确输入输出">1. 明确输入输出</h2>
<h3 id="将输入转化为vector">将输入转化为vector</h3>
<ol>
<li><strong>文本翻译</strong> &ndash; 将词汇转化成same length vector（word2vector，one-hot coding）</li>
<li><strong>信号处理</strong> &ndash; 生成一段时长的window 将其中内容转化为一个frame vector 然后跳一个gap向前</li>
<li><strong>图(graph)</strong> &ndash; 每一个node的信息汇总成vector vertices单独拎出来和后面attention matrix结合</li>
<li><strong>图像</strong> &ndash; 每一个pixel有三通道的向量 <strong>DETR</strong></li>
</ol>
<h3 id="明确输出类型">明确输出类型</h3>
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302011214326.png" alt="image-20230201121424237" style="zoom:50%;" />
<p>例子：文本处理词性标注（POS tagging） / 一句话的感情分析（sentiment analysis） /  model自己选择输出大小</p>
<hr>
<h2 id="第一类问题sequence-labeling">第一类问题（Sequence Labeling）</h2>
<h3 id="1-面对的问题">1. 面对的问题</h3>
<h4 id="1给一个sequence-如何生成corresponding-sequence--labels">（1）给一个sequence 如何生成corresponding sequence / labels</h4>
<p>如果使用单一的fully-connected layers，那么给定的sequence长度随着时间变化，则无法做到泛化</p>
<p>因此我们需要一个flexible的方法 不用限定sequence的长度</p>
<h4 id="2如何体现sequence的上下文信息对当前vector的output的影响">（2）如何体现sequence的上下文信息对当前vector的output的影响</h4>
<p>双向RNN能够体现上下文 用一个window将需要考虑的相邻向量容纳进去 但是无法平行计算(parallel &ndash; speedup)</p>
<h3 id="2-self-attention">2. Self-attention</h3>
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302011215795.png" alt="image-20230201121536700" style="zoom:50%;" />
<h4 id="1-解决的方法">（1） 解决的方法</h4>
<h5 id="1-attention-score-alpha----体现上下文信息">1. Attention Score $\alpha$ &ndash; 体现上下文信息</h5>
<p>每个input向量之间会计算一个attention score 用来表示相似性 （计算方法有很多例如dot-product / addictive）</p>
<h5 id="2-引入positional-encoding">2. 引入positional encoding</h5>
<p>针对每一个attention layer 无论sequence的顺序如何，他都不影响结果输出，这表示<strong>no position information in self-attention</strong> &ndash; 天涯若比邻 每个向量之间的距离都是“一样的”</p>
<p><em><strong>引入unique positional vector $e^i$</strong></em></p>
<p><a href="https://arxiv.org/abs/2003.09229" target="_blank" rel="noopener">Positional encoding article &ndash; Learning to Encode Position for Transformer with Continuous Dynamical Model</a></p>
<h4 id="2-流程">（2） 流程</h4>
<h5 id="1-生成每一个向量的query-key-value">1. 生成每一个向量的Query Key Value</h5>
<p>每一层attention layer共享一个Query Key Value matrix $$W^q \ W_k \ W_v$$</p>
<p>引入三个变量是为了引入更多可学习的参数，但是共享参数是为了减少训练量</p>
<h5 id="2-q-k-组合生成-alpha--soft-max-后生成-alphaprime">2. Q K 组合生成 $\alpha$  Soft-max 后生成 $\alpha^{\prime}$</h5>
<p>下图为第一个向量与sequence中所有向量的attention score计算示意图</p>
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302011215171.png" alt="image-20230131095641605" style="zoom:50%;" />
<h5 id="3-extract-information-based-on-attention-scores">3. Extract information based on attention scores</h5>
<p>$$
\boldsymbol{b}^{\mathbf{1}}=\sum_{i} \alpha_{1, i}^{\prime} \boldsymbol{v}^{i}
$$</p>
<h4 id="3-matrix-representation">（3） Matrix representation</h4>
<div align=center><img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302011215100.png" alt="image-20230131100815558" style="zoom:50%;" />
</div>
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302011216960.png" alt="image-20230131101707040" style="zoom:50%;" />
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302011216302.png" alt="image-20230131101738724" style="zoom:50%;" />
$$
\begin{aligned}
Q &= w^q · I \\
K &= w^k · I \\  
V &= w^v · I \\ 
A^{\prime} = softmax(A) &= softmax(K^T Q) \\
O &= V A^{\prime}
\end{aligned}
$$
<h3 id="3-multi-head-self-attention">3. Multi-head Self-attention</h3>
<p>different types of relevance</p>
<h3 id="4-self-attention-vs-cnn">4. Self-attention VS CNN</h3>
<p>CNN: self-attention that can only attends in a receptive field</p>
<p>self-attention: CNN with learnable receptive field (complex version of CNN)</p>
<p><a href="https://arxiv.org/abs/1911.03584" target="_blank" rel="noopener">On the Relationship between Self-Attention and Convolutional Layers</a></p>
<p>小样本数据集上CNN模型小更高效 大数据集样本上self-attention往往能找到更多的connection</p>
<h3 id="5-self-attention-vs-rnn">5. Self-attention VS RNN</h3>
<p>RNN递进关系会导致远距离很难考虑 并且 不是平行架构无法加速</p>
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302011216327.png" alt="image-20230131111746795" style="zoom:50%;" />
<p><a href="https://arxiv.org/abs/2006.16236" target="_blank" rel="noopener">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</a></p>
<h3 id="6-综述文章">6. 综述文章</h3>
<p><a href="https://arxiv.org/abs/2009.06732" target="_blank" rel="noopener">Efficient Transformers: A Survey</a></p>
<p><a href="https://arxiv.org/abs/2011.04006" target="_blank" rel="noopener">Long Range Arena: A Benchmark for Efficient Transformers</a></p>
<hr>
<h2 id="transformers">Transformers</h2>
<hr>
<h2 id="sequence-to-sequenceseq2seq">Sequence-to-sequence(seq2seq)</h2>
<blockquote>
<p>Input a sequence, output a sequence.</p>
<p>The output length is determined by model.</p>
</blockquote>
<p><a href="https://arxiv.org/abs/2005.12872" target="_blank" rel="noopener">seq2seq for object detection &ndash; End-to-End Object Detection with Transformers</a></p>
<div align=center><img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302020934811.png" alt="image-20230202093420708" style="zoom:33%;" />
</div>
<h2 id="transformers-encoder----self-attention">Transformer&rsquo;s Encoder &ndash; Self-attention</h2>
<blockquote>
<p>encoder的核心问题 是将一个sequence的向量如何转化为同样长度的向量</p>
</blockquote>
<div align=center>
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302020943672.png" alt="image-20230202094355618" style="zoom:50%;" />
</div>
<h3 id="每一个block的处理顺序">每一个block的处理顺序</h3>
<ol>
<li>
<p>self-attention</p>
</li>
<li>
<p>residual connection （防止梯度消失）</p>
</li>
</ol>
<blockquote>
<p>为什么residual block能够防止梯度消失？</p>
<div align=center>
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302021804498.png" alt="image-20230202180404456" style="zoom:80%;" />
</div>
<p>根据后向传播的链式法则，</p>
</blockquote>
<p>$$
\frac{\partial L}{\partial X_{\text{Aout}}} = \frac{\partial L}{\partial X_{\text{Din}}} \cdot \frac{\partial X_{\text{Din}}}{\partial X_{\text{Aout}}}
$$
$$
\because X_{\text{Din}} = X_{\text{Aout}} + C(B(X_{\text{Aout}}))
$$
$$
\frac{\partial L}{\partial X_{\text{Aout}}} = \frac{\partial L}{\partial X_{\text{Din}}} \cdot \left[ 1 + \frac{\partial X_{\text{Din}}}{\partial X_{\text{Cout}}} \frac{\partial X_{\text{Cout}}}{\partial X_{\text{Bout}}} \frac{\partial X_{\text{Bout}}}{\partial X_{\text{Aout}}}\right]
$$
$$
= \frac{\partial L}{\partial X_{\text{Din}}} + \frac{\partial L}{\partial X_{\text{Din}}}\frac{\partial X_{\text{Din}}}{\partial X_{\text{Cout}}} \frac{\partial X_{\text{Cout}}}{\partial X_{\text{Bout}}} \frac{\partial X_{\text{Bout}}}{\partial X_{\text{Aout}}}
$$
$$
= \frac{\partial L}{\partial X_{\text{Din}}} + \text {original loss gradient}
$$</p>
<blockquote>
<p>通常微分后都小于1 故原始梯度容易越算越小 到前端就容易消失无法改变</p>
</blockquote>
<ol start="3">
<li>norm （layer normalization）同一个向量的不同dimension计算mean和deviation</li>
</ol>
<p>$$
\begin{bmatrix}
x_1 &amp; x_2 &amp; \cdots &amp; x_K
\end{bmatrix}^{T}
\rightarrow
\begin{bmatrix}
x_1^{\prime} &amp; x_2^{\prime} &amp; \cdots &amp; x_K^{\prime}
\end{bmatrix}^{T}
$$</p>
<blockquote>
<p>batch normalization是对同一个dimension不同的feature的mean和variance（不同向量之间的$x_i$的$\mu$）</p>
</blockquote>
<p>[以下是encoder的详细说明](#Transformer&rsquo;s Encoder &ndash; Self-attention)</p>
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302020956666.png" alt="image-20230202095606545" style="zoom:50%;" />
<h3 id="encoder架构的变形">Encoder架构的变形</h3>
<ul>
<li>[<a href="https://arxiv.org/abs/2002.04745" target="_blank" rel="noopener">2002.04745] On Layer Normalization in the Transformer Architecture (arxiv.org)</a></li>
<li>[<a href="https://arxiv.org/abs/2003.07845" target="_blank" rel="noopener">2003.07845] PowerNorm: Rethinking Batch Normalization in Transformers (arxiv.org)</a></li>
</ul>
<h2 id="transformers-decoder">Transformer&rsquo;s decoder</h2>
<h3 id="输出方式----autoregressive-和-non-autoregressive">输出方式 &ndash; Autoregressive 和 non-autoregressive</h3>
<ul>
<li>将decoder的sequence vectors输入decoder</li>
<li>autoregressive &ndash; 决定了decoder的输出方式（AT）</li>
</ul>
<blockquote>
<ol>
<li>首先在decoder里输入vectors</li>
<li>输入begin of sentence（通常为one-hot coding vector）</li>
<li>第一个输出为一个长度为想要输出的范围的size长度的vector 再经过softmax得到distribution 取最大值</li>
<li>再将第一个输出当成新的输入（改成one-hot coding） 此时decoder输入有bos和之前的输出</li>
<li>再得到新的输出</li>
<li>以此类推</li>
</ol>
</blockquote>
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302021015182.png" alt="image-20230202101556039" style="zoom:50%;" />
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302021020250.png" alt="image-20230202102046116" style="zoom:50%;" />
<h3 id="decoder-本身架构">Decoder 本身架构</h3>
<ul>
<li>抛去中间的block 可以看到就是与decoder 相似的主体结构 唯一不同的是masked self-attention，很好理解不能剧透后面的部分。(类似于RNN)</li>
</ul>
<div align=center><img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302021639103.png" alt="image-20230202163936914" style="zoom:50%;" />
</div>
<blockquote>
<p>eg.</p>
<div align=center><img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302021641078.png" alt="image-20230202164134997" style="zoom: 33%;" />
</div>
</blockquote>
<h3 id="encoder和decoder如何交互">Encoder和decoder如何交互？</h3>
<div align=center><img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302021655592.png" alt="image-20230202165550501" style="zoom:45%;" />
</div>
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302021700358.png" alt="image-20230202170044216" style="zoom: 67%;" />
<ul>
<li>cross layer 可以用不同类型</li>
</ul>
<blockquote>
<p>[<a href="https://arxiv.org/abs/2005.08081" target="_blank" rel="noopener">2005.08081] Rethinking and Improving Natural Language Generation with Layer-Wise Multi-View Decoding (arxiv.org)</a></p>
</blockquote>
<h2 id="training">Training</h2>
<p><em><strong>Loss: 每一个词汇的ground truth 和 decoder inference 之后的结果 计算cross entropy</strong></em></p>
<h3 id="teacher-forcing">Teacher Forcing</h3>
<p>为了更好的训练decoder 防止encoder的error propagation 所以会使用把正确结果放进decoder的输入看能不能输出正确结果</p>
<img src="https://raw.githubusercontent.com/keeplearning-again/Typora_blog_images/main/blog/202302021749300.png" alt="image-20230202174931088" style="zoom: 50%;" />
<h3 id="scheduled-sampling">Scheduled Sampling</h3>
<p>为了避免没有泛化能力 就给decoder的输入放一些故意的错误（原理有点类似于神经网路 dropout）</p>
<blockquote>
<p>[<a href="https://arxiv.org/abs/1906.07651" target="_blank" rel="noopener">1906.07651] Scheduled Sampling for Transformers (arxiv.org)</a></p>
<p>[<a href="https://arxiv.org/abs/1906.04331" target="_blank" rel="noopener">1906.04331] Parallel Scheduled Sampling (arxiv.org)</a></p>
</blockquote>
<h2 id="tips">Tips</h2>
<ol>
<li>
<p>copy mechanism</p>
</li>
<li>
<p>guided attention ？</p>
</li>
<li>
<p>Beam Search ？</p>
</li>
<li>
<p>Optimizing evaluation metrics？ &ndash; BLEU score</p>
<ol>
<li>只能用在testing 的evaluation 不能用在loss是因为不可微 不能反向传递</li>
<li>when you don&rsquo;t know how to optimize, just use reinforcement learning!!!</li>
</ol>
<p>把BLEU score看成reward 把decoder看成agent</p>
<blockquote>
<p>[<a href="https://arxiv.org/abs/1511.06732" target="_blank" rel="noopener">1511.06732] Sequence Level Training with Recurrent Neural Networks (arxiv.org)</a></p>
</blockquote>
</li>
</ol>
<hr>
<p>图片来源：</p>
<p><a href="https://www.bilibili.com/video/BV1v3411r78R?p=1&amp;vd_source=0da602efaef9a75c3e62c481d182f95c" target="_blank" rel="noopener">10.【李宏毅机器学习2021】自注意力机制 (Self-attention) (上)_哔哩哔哩_bilibili</a></p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/academic/">Academic</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fkeeplearning-again.github.io%2Fpost%2Ftransformer%2F&amp;text=Self-attention&#43;mechanism&#43;and&#43;Transformers" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fkeeplearning-again.github.io%2Fpost%2Ftransformer%2F&amp;t=Self-attention&#43;mechanism&#43;and&#43;Transformers" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=Self-attention%20mechanism%20and%20Transformers&amp;body=https%3A%2F%2Fkeeplearning-again.github.io%2Fpost%2Ftransformer%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fkeeplearning-again.github.io%2Fpost%2Ftransformer%2F&amp;title=Self-attention&#43;mechanism&#43;and&#43;Transformers" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=Self-attention&#43;mechanism&#43;and&#43;Transformers%20https%3A%2F%2Fkeeplearning-again.github.io%2Fpost%2Ftransformer%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fkeeplearning-again.github.io%2Fpost%2Ftransformer%2F&amp;title=Self-attention&#43;mechanism&#43;and&#43;Transformers" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://keeplearning-again.github.io/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu77d434e1653c7c85e21fab9a7004fc26_620242_270x270_fill_q75_lanczos_center.jpg" alt="Ruiqiang Xiao"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://keeplearning-again.github.io/">Ruiqiang Xiao</a></h5>
      <h6 class="card-subtitle">MSc student in HKUST</h6>
      <p class="card-text">My research interests include distributed robotics, mobile computing and programmable matter.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:rxiaoad@connect.ust.hk" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/GeorgeCushen" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/keeplearning-again" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/uploads/resume.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
    




  
















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2023 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js"></script>




  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.3322c0d94f0e691b0b24c63f4c41064b.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>


















</body>
</html>
